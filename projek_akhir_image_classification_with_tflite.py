# -*- coding: utf-8 -*-
"""Projek Akhir: Image Classification with TFLite.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZUO5d3iwYYoQwnZhUvBaeRDoISo6eoBt

# Project Name: Image Classification with TFLite

## Part 1: Dev's Profile

Nama: Tariq Fitria Aziz

Tanggal Bergabung: 2 September 2020

Asal: Wonogiri, Jawa Tengah

## Part 2: Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers import Nadam
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint

from google.colab import files

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline

import zipfile, gzip
import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator

"""## Part 3: Data Preparation

Dataset yang digunakan di sini adalah dataset "ASL Alphabet Language" yang dapat diunduh dari Kaggle. Dataset ini memiliki 87.000 gambar dengan resolusi 200x200 piksel, dengan total 29 kelas. <br>

<b>Sumber: https://www.kaggle.com/grassknoted/asl-alphabet

### Load Kaggle Dataset
"""

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download grassknoted/asl-alphabet

"""### Mengekstrak Dataset"""

zipfile_name = '/content/asl-alphabet.zip'
zip_read = zipfile.ZipFile(zipfile_name, 'r')
zip_read.extractall('/content')
zip_read.close()

train_dir = os.path.join("/content/asl_alphabet_train/asl_alphabet_train")

"""## Part 4: Image Augmentation

### Image Data Generator
"""

train_datagen = ImageDataGenerator(
    rescale = 1./255,
    brightness_range = [0.7, 1.3],
    validation_split = 0.2
)

train_datagen2 = ImageDataGenerator(
    rescale = 1./255,
    brightness_range = [0.7, 1.3],
    preprocessing_function = preprocess_input,
    validation_split = 0.2
)

"""### Persiapkan Data Latih dan Validasi"""

seed = 42

train_generator = train_datagen.flow_from_directory(
        train_dir, 
        target_size = (150, 150), 
        shuffle=True,
        seed = seed,
        batch_size = 16,
        class_mode ='categorical',
        subset = 'training'
        )

train_generator2 = train_datagen2.flow_from_directory(
        train_dir,  
        target_size = (150, 150), 
        shuffle=True,
        seed = seed,
        batch_size = 16,
        class_mode ='categorical',
        subset = 'training'
        )

validation_generator = train_datagen.flow_from_directory(
        train_dir, 
        target_size = (150, 150),
        shuffle=True,
        seed = seed,
        batch_size = 16,
        class_mode ='categorical',
        subset = 'validation'
        )

validation_generator2 = train_datagen2.flow_from_directory(
        train_dir, 
        target_size = (150, 150), 
        shuffle=True,
        seed = seed,
        batch_size = 16,
        class_mode ='categorical',
        subset = 'validation'
        )

"""## Part 5: Create Model

### Transfer Learning Model
"""

base_model = VGG16(weights='imagenet',include_top=False, input_shape=(150,150,3), pooling='max')

base_model.trainable = False

base_model.summary()

"""### Model Function"""

def model_initialize(model_name, optimizer):
    model = Sequential(name = model_name)
    
    model.add(Conv2D(
        filters = 32, 
        kernel_size = (3,3), 
        activation = 'relu', 
        kernel_initializer="he_uniform",
        padding="same", 
        input_shape = (150, 150, 3)
        )
    )
    model.add(MaxPooling2D(
        pool_size = (2, 2)
        )
    )
    model.add(Conv2D(
        filters = 64, 
        kernel_size = (3,3),
        activation = 'relu',
        padding="same",
        kernel_initializer="he_uniform"
        )
    )
    model.add(MaxPooling2D(
        pool_size = (2, 2)
        )
    )
    model.add(Conv2D(
        filters = 128, 
        kernel_size = (3,3),
        activation = 'relu',
        padding="same",
        kernel_initializer="he_uniform"
        )
    )
    model.add(MaxPooling2D(
        pool_size = (2, 2)
        )
    )
    model.add(Conv2D(
        filters = 256, 
        kernel_size = (3,3),
        activation = 'relu',
        padding="same",
        kernel_initializer="he_uniform"
        )
    )
    model.add(MaxPooling2D(
        pool_size = (2, 2)
        )
    )
    model.add(Flatten())
    
    model.add(Dense(units = 4096, activation = 'relu', kernel_initializer="he_uniform"))
    model.add(Dropout(rate = 0.5))
    model.add(Dense(units = 2048, activation = 'relu', kernel_initializer="he_uniform"))
    model.add(Dropout(rate = 0.5))
    model.add(Dense(units = 1024, activation = 'relu', kernel_initializer="he_uniform"))
    model.add(Dropout(rate = 0.5))
    model.add(Dense(units = 512, activation = 'relu', kernel_initializer="he_uniform"))
    model.add(Dropout(rate = 0.5))
    model.add(Dense(units = 256, activation = 'relu', kernel_initializer="he_uniform"))
    model.add(Dropout(rate = 0.5))
    model.add(Dense(units = 128, activation = 'relu', kernel_initializer="he_uniform"))
    model.add(Dropout(rate = 0.5))
    model.add(Dense(units = 29, activation = 'softmax'))

    model.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics=['accuracy'])

    return model

def model_with_tflearning(model_name, optimizer):
  model = Sequential(name = model_name)

  model.add(base_model)
  
  model.add(Flatten()) 
  model.add(Dense(units = 2048, activation = 'relu', kernel_initializer="he_uniform"))
  model.add(Dropout(rate = 0.5))
  model.add(Dense(units = 1024, activation = 'relu', kernel_initializer="he_uniform"))
  model.add(Dropout(rate = 0.5))
  model.add(Dense(units = 512, activation = 'relu', kernel_initializer="he_uniform"))
  model.add(Dropout(rate = 0.5))
  model.add(Dense(units = 256, activation = 'relu', kernel_initializer="he_uniform"))
  model.add(Dropout(rate = 0.5))
  model.add(Dense(units = 29, activation = 'softmax'))

  model.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics=['accuracy'])

  return model

"""### Initialize Model"""

opt = Nadam(learning_rate=0.0001)
opt2 = Nadam(learning_rate = 0.0001)

Classifier1 = model_initialize('ImageClassifier',optimizer=opt)

Classifier2 = model_with_tflearning('ImageClassifier_with_VGG16', optimizer=opt2)

"""### Model Summary"""

Classifier1.summary()

Classifier2.summary()

"""## Part 6: Model Training

### Define Callbacks
"""

es1 = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 20)
mc1 = ModelCheckpoint('best_model.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)

es2 = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 10)
mc2 = ModelCheckpoint('best_model2.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)

"""### Train Model"""

history1 = Classifier1.fit(
    train_generator, 
    steps_per_epoch = 80,
    epochs = 250, 
    validation_data = validation_generator,
    validation_steps = 20,
    verbose = 1,
    callbacks=[es1, mc1]
    )

history2 = Classifier2.fit(
    train_generator2, 
    steps_per_epoch = 80,
    epochs = 200, 
    validation_data = validation_generator2,
    validation_steps = 20,
    verbose = 1,
    callbacks=[es2, mc2]
    )

"""### Training Results"""

plt.plot(history1.history['accuracy'], label='Train')
plt.plot(history1.history['val_accuracy'], label='Val')

plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.title("Plot Accuracy Classifier 1")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.plot(history2.history['accuracy'], label='Train')
plt.plot(history2.history['val_accuracy'], label='Val')

plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.title("Plot Accuracy Classifier 2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.plot(history1.history['loss'], label='Train')
plt.plot(history1.history['val_loss'], label='Val')

plt.xlabel('epochs')
plt.ylabel('loss')
plt.title("Plot Loss Classifier 1")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.plot(history2.history['loss'], label='Train')
plt.plot(history2.history['val_loss'], label='Val')

plt.xlabel('epochs')
plt.ylabel('loss')
plt.title("Plot Loss Classifier  2")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Fine Tuning"""

base_model.trainable = True

# Tampillkan Jumlah Layer pada Model
print("Jumlah Layer pada Model Classifier 2 adalah {}".format(len(base_model.layers)))

# Fine Tune dimulai pada layer 16
start_fine_tune = 16
for i in base_model.layers[:start_fine_tune]:
  i.trainable = False

opt2 = RMSprop(learning_rate = 0.0001)
Classifier2.compile(loss='categorical_crossentropy', optimizer = opt2, metrics=['accuracy'])

Classifier2.summary()

es3 = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 10)
mc3 = ModelCheckpoint('best_model3.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)

history3 = Classifier2.fit(
    train_generator2, 
    steps_per_epoch = 80,
    epochs = 100, 
    validation_data = validation_generator2,
    validation_steps = 20,
    verbose = 1,
    callbacks=[es3, mc3]
    )

plt.plot(history3.history['accuracy'], label='Train')
plt.plot(history3.history['val_accuracy'], label='Val')

plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.title("Plot Accuracy Classifier 2 After Fine Tuning")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.plot(history3.history['loss'], label='Train')
plt.plot(history3.history['val_loss'], label='Val')

plt.xlabel('epochs')
plt.ylabel('loss')
plt.title("Plot Loss Classifier  2 After Fine Tuning")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""## Part 7: Test Model"""

print(train_generator.class_indices)

best_model_dir1 = '/content/best_model.h5'
model1 = load_model(best_model_dir1)

best_model_dir2 = '/content/best_model3.h5'
model2 = load_model(best_model_dir2)

def visualize_image(path, model, asl_name):
  fig, xfig = plt.subplots()
  img = image.load_img(path, target_size=(150,150))
  imgplot = xfig.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  classes = model.predict(images, batch_size=16)
  output_class = np.argmax(classes)
  
  if output_class == 0:
    xfig.set_title(asl_name + " | Pred: A")
  elif output_class == 1:
    xfig.set_title(asl_name + " | Pred: B")
  elif output_class == 2:
    xfig.set_title(asl_name + " | Pred: C")
  elif output_class == 3:
    xfig.set_title(asl_name + " | Pred: D")
  elif output_class == 4:
    xfig.set_title(asl_name + " | Pred: E")
  elif output_class == 5:
    xfig.set_title(asl_name + " | Pred: F")
  elif output_class == 6:
    xfig.set_title(asl_name + " | Pred: G")
  elif output_class == 7:
    xfig.set_title(asl_name + " | Pred: H")
  elif output_class == 8:
    xfig.set_title(asl_name + " | Pred: I")
  elif output_class == 9:
    xfig.set_title(asl_name + " | Pred: J")
  elif output_class == 10:
    xfig.set_title(asl_name + " | Pred: K")
  elif output_class == 11:
    xfig.set_title(asl_name + " | Pred: L")
  elif output_class == 12:
    xfig.set_title(asl_name + " | Pred: M")
  elif output_class == 13:
    xfig.set_title(asl_name + " | Pred: N")
  elif output_class == 14:
    xfig.set_title(asl_name + " | Pred: O")
  elif output_class == 15:
    xfig.set_title(asl_name + " | Pred: P")
  elif output_class == 16:
    xfig.set_title(asl_name + " | Pred: Q")
  elif output_class == 17:
    xfig.set_title(asl_name + " | Pred: R")
  elif output_class == 18:
    xfig.set_title(asl_name + " | Pred: S")
  elif output_class == 19:
    xfig.set_title(asl_name + " | Pred: T")
  elif output_class == 20:
    xfig.set_title(asl_name + " | Pred: U")
  elif output_class == 21:
    xfig.set_title(asl_name + " | Pred: V")
  elif output_class == 22:
    xfig.set_title(asl_name + " | Pred: W")
  elif output_class == 23:
    xfig.set_title(asl_name + " | Pred: X")
  elif output_class == 24:
    xfig.set_title(asl_name + " | Pred: Y")
  elif output_class == 25:
    xfig.set_title(asl_name + " | Pred: Z")
  elif output_class == 26:
    xfig.set_title(asl_name + " | Pred: Del")
  elif output_class == 27:
    xfig.set_title(asl_name + " | Pred: Nothing")
  elif output_class == 28:
    xfig.set_title(asl_name + " | Pred: Space")

"""### Classifier 1"""

test_dir = '/content/asl_alphabet_test/asl_alphabet_test'
for i in os.listdir(test_dir):
  path = os.path.join(test_dir, i)
  visualize_image(path=path, model=model1, asl_name=i)

"""### Classifier 2"""

for i in os.listdir(test_dir):
  path = os.path.join(test_dir, i)
  visualize_image(path=path, model=model2, asl_name=i)

"""## Kesimpulan

Di sini, telah dibuat 2 model image classifier dengan struktur model berbeda. Model pertama adalah model yang dibuat dan dilatih dari nol dengan memanfaatkan beberapa layer seperti Conv2D, MaxPooling, dan Dense. Sementara model kedua memanfaatkan pre-trained model VGG16 dan dilakukan transfer learning serta fine-tuning. <br>

Dari proses pelatihan, dapat dilihat bahwa model Classifier 1 mampu mencapai level akurasi training di atas 80%, tetapi hanya mampu meraih akurasi validasi di angka 75%. Untuk Classifier 2, dengan transfer learning dan fine-tuning, nilai akurasi training dan validasi di atas 90%, sehingga model kedua terlihat lebih superior.<br>

Hal ini sesuai dengan saat pengetesan. Pada saat pengetesan untuk memprediksi gambar, dapat dilihat bahwa model 1 (Classifier 1) mengalami salah prediksi sebanyak 5 kali, tetapi model 2 (Classifier 2) hanya mengalami kesalahan 1 kali. Dengan begitu, model 2 lebih diunggulkan untuk diaplikasikan pada sistem klasifikasi pada android.<br>

## Konversi Ke TFLite
"""

# Konversi model.
converter1 = tf.lite.TFLiteConverter.from_keras_model(model1)
tflite_model1 = converter1.convert()

with tf.io.gfile.GFile('model.tflite', 'wb') as f:
  f.write(tflite_model1)

converter2 = tf.lite.TFLiteConverter.from_keras_model(model2)
tflite_model2 = converter2.convert()

with tf.io.gfile.GFile('model.tflite2', 'wb') as f:
  f.write(tflite_model2)