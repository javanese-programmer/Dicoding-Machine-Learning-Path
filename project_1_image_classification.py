# -*- coding: utf-8 -*-
"""Project 1: Image Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dZCCOfvW1b3ouKCzckZfq0AQ9QasaP_b

# Project Name: Image Classification

## Part 1: Dev's Profile

Nama: Tariq Fitria Aziz

Tanggal Bergabung: 2 September 2020

Asal: Wonogiri, Jawa Tengah

## Part 2: Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV

import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint

from google.colab import files

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline

import time
import zipfile, gzip
import os
import shutil
import random
from tensorflow.keras.preprocessing.image import ImageDataGenerator

"""## Part 3: Data Preparation

### Mengunduh Dataset
"""

!wget https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip -O /tmp/rockpaperscissors.zip

"""### Mengekstrak Dataset"""

zipfile_name = '/tmp/rockpaperscissors.zip'
zip_read = zipfile.ZipFile(zipfile_name, 'r')
zip_read.extractall('/tmp')
zip_read.close()

"""### Membuat Direktori"""

direktori_gambar = '/tmp/rockpaperscissors/rps-cv-images'
direktori_dasar = '/tmp/rockpaperscissors'

rock_images_dir = '/tmp/rockpaperscissors/rps-cv-images/rock'
paper_images_dir = '/tmp/rockpaperscissors/rps-cv-images/paper'
scissors_images_dir = '/tmp/rockpaperscissors/rps-cv-images/scissors'

train_dir = os.path.join(direktori_dasar, 'train')
val_dir = os.path.join(direktori_dasar, 'val')

os.mkdir(train_dir) 
os.mkdir(val_dir)

train_rock_dir = os.path.join(train_dir, 'rock')
val_rock_dir = os.path.join(val_dir, 'rock')

train_paper_dir = os.path.join(train_dir, 'paper')
val_paper_dir = os.path.join(val_dir, 'paper')

train_scissors_dir = os.path.join(train_dir, 'scissors')
val_scissors_dir = os.path.join(val_dir, 'scissors')

os.mkdir(train_rock_dir) 
os.mkdir(val_rock_dir)

os.mkdir(train_paper_dir) 
os.mkdir(val_paper_dir)

os.mkdir(train_scissors_dir) 
os.mkdir(val_scissors_dir)

"""### Membagi Dataset"""

train_rock, val_rock = train_test_split(os.listdir(rock_images_dir), test_size= 0.3994515539, random_state = 42)
train_paper, val_paper = train_test_split(os.listdir(paper_images_dir), test_size= 0.3994515539, random_state = 42)
train_scissors, val_scissors = train_test_split(os.listdir(scissors_images_dir), test_size= 0.3994515539, random_state = 42)

for i in train_rock:
  shutil.move(os.path.join(rock_images_dir, i), train_rock_dir)
for i in train_paper:
  shutil.move(os.path.join(paper_images_dir, i), train_paper_dir)
for i in train_scissors:
  shutil.move(os.path.join(scissors_images_dir, i), train_scissors_dir)

k = 0
while(k<2):
  k += 1
  choice = random.choice(range(1,4))
  if(choice == 1):
    val_move = random.choice(val_rock)
    shutil.move(os.path.join(rock_images_dir, val_move), train_rock_dir)
    val_rock.remove(val_move)
  elif(choice == 2):
    val_move = random.choice(val_paper)
    shutil.move(os.path.join(paper_images_dir, val_move), train_paper_dir)
    val_paper.remove(val_move)
  else:
    val_move = random.choice(val_scissors)
    shutil.move(os.path.join(scissors_images_dir, val_move), train_scissors_dir)
    val_scissors.remove(val_move)

for i in val_rock:
  shutil.move(os.path.join(rock_images_dir, i), val_rock_dir)
for i in val_paper:
  shutil.move(os.path.join(paper_images_dir, i), val_paper_dir)
for i in val_scissors:
  shutil.move(os.path.join(scissors_images_dir, i), val_scissors_dir)

"""## Part 4: Image Augmentation

### Image Data Generator
"""

train_datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range = 25,
    horizontal_flip = True,
    shear_range = 0.25,
    zoom_range = 0.25,
    fill_mode = 'reflect',
    width_shift_range = 0.25,
    height_shift_range = 0.25
)

test_datagen = ImageDataGenerator(
                    rescale=1./255,
                    )

"""### Persiapkan Data Latih dan Validasi"""

train_generator = train_datagen.flow_from_directory(
        train_dir,  
        target_size = (150, 150), 
        batch_size = 16,
        class_mode ='categorical',
        )

validation_generator = test_datagen.flow_from_directory(
        val_dir, 
        target_size = (150, 150), 
        batch_size = 16,
        class_mode ='categorical')

"""## Part 5: Create Model

### Model Function
"""

def model_initialize(model_name, optimizer):
    model = Sequential(name = model_name)
    
    model.add(Conv2D(
        filters = 32, 
        kernel_size = (3,3), 
        activation = 'relu', 
        kernel_initializer="he_uniform", 
        input_shape = (150, 150, 3)
        )
    )
    model.add(MaxPooling2D(
        pool_size = (2, 2)
        )
    )
    model.add(Conv2D(
        filters = 64, 
        kernel_size = (3,3),
        activation = 'relu',
        kernel_initializer="he_uniform"
        )
    )
    model.add(MaxPooling2D(
        pool_size = (2, 2)
        )
    )
    model.add(Conv2D(
        filters = 128, 
        kernel_size = (3,3),
        activation = 'relu',
        kernel_initializer="he_uniform"
        )
    )
    model.add(MaxPooling2D(
        pool_size = (2, 2)
        )
    )
    model.add(Conv2D(
        filters = 256, 
        kernel_size = (3,3),
        activation = 'relu',
        kernel_initializer="he_uniform"
        )
    )
    model.add(MaxPooling2D(
        pool_size = (2, 2)
        )
    )
    model.add(Flatten())
    
    model.add(Dense(units = 512, activation = 'relu', kernel_initializer="he_uniform"))
    model.add(Dropout(rate = 0.2))
    model.add(Dense(units = 256, activation = 'relu', kernel_initializer="he_uniform"))
    model.add(Dropout(rate = 0.1))
    model.add(Dense(units = 64, activation = 'relu', kernel_initializer="he_uniform"))
    model.add(Dropout(rate = 0.1))
    model.add(Dense(units = 3, activation = 'softmax'))

    model.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics=['accuracy'])

    return model

def model_initialize2(model_name, optimizer):
    model = Sequential(name = model_name)
    
    model.add(Conv2D(
        filters = 32, 
        kernel_size = (3,3), 
        activation = 'relu', 
        kernel_initializer="he_uniform", 
        input_shape = (150, 150, 3)
        )
    )
    model.add(MaxPooling2D(
        pool_size = (2, 2)
        )
    )
    model.add(Conv2D(
        filters = 64, 
        kernel_size = (3,3),
        activation = 'relu',
        kernel_initializer="he_uniform"
        )
    )
    model.add(MaxPooling2D(
        pool_size = (2, 2)
        )
    )
    model.add(Conv2D(
        filters = 128, 
        kernel_size = (3,3),
        activation = 'relu',
        kernel_initializer="he_uniform"
        )
    )
    model.add(MaxPooling2D(
        pool_size = (2, 2)
        )
    )
    model.add(Conv2D(
        filters = 256, 
        kernel_size = (3,3),
        activation = 'relu',
        kernel_initializer="he_uniform"
        )
    )
    model.add(MaxPooling2D(
        pool_size = (2, 2)
        )
    )
    model.add(Conv2D(
        filters = 512, 
        kernel_size = (3,3),
        activation = 'relu',
        kernel_initializer="he_uniform"
        )
    )
    model.add(MaxPooling2D(
        pool_size = (2, 2)
        )
    )
    model.add(Flatten())
    
    model.add(Dense(units = 512, activation = 'relu', kernel_initializer="he_uniform"))
    model.add(Dropout(rate = 0.2))
    model.add(Dense(units = 256, activation = 'relu', kernel_initializer="he_uniform"))
    model.add(Dropout(rate = 0.1))
    model.add(Dense(units = 128, activation = 'relu', kernel_initializer="he_uniform"))
    model.add(Dropout(rate = 0.1))
    model.add(Dense(units = 64, activation = 'relu', kernel_initializer="he_uniform"))
    model.add(Dropout(rate = 0.1))
    model.add(Dense(units = 3, activation = 'softmax'))

    model.compile(loss='categorical_crossentropy', optimizer = optimizer, metrics=['accuracy'])

    return model

"""### Initialize Model"""

Classifier1 = model_initialize('ImageClassifier1','rmsprop')

Classifier2 = model_initialize('ImageClassifier2','nadam')

Classifier3 = model_initialize('ImageClassifier3','adamax')

Classifier4 = model_initialize2('ImageClassifier3','adamax')

"""### Model Summary"""

Classifier1.summary()

Classifier2.summary()

Classifier3.summary()

Classifier4.summary()

"""## Part 6: Model Training

### Define Callbacks
"""

es1 = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 4)

mc1 = ModelCheckpoint('best_model.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)

mc2 = ModelCheckpoint('best_model2.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)

mc3 = ModelCheckpoint('best_model3.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)

mc4 = ModelCheckpoint('best_model4.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)

"""### Train Model

Classifier 1
"""

starttime1 = time.time()

history1 = Classifier1.fit(
    train_generator, 
    steps_per_epoch = 80,
    epochs = 20, 
    validation_data = validation_generator,
    validation_steps = 20,
    verbose = 1,
    callbacks=[es1, mc1]
    )

endtime1 = time.time()
elapsedtime1 = endtime1 - starttime1

"""Classifier 2"""

starttime2 = time.time()

history2 = Classifier2.fit(
    train_generator, 
    steps_per_epoch = 80,
    epochs = 20, 
    validation_data = validation_generator,
    validation_steps = 20,
    verbose = 1,
    callbacks=[es1, mc2]
    )

endtime2 = time.time()
elapsedtime2 = endtime2 - starttime2

"""Classifier 3"""

starttime3 = time.time()

history3 = Classifier3.fit(
    train_generator, 
    steps_per_epoch = 80,
    epochs = 20, 
    validation_data = validation_generator,
    validation_steps = 20,
    verbose = 1,
    callbacks=[es1, mc3]
    )

endtime3 = time.time()
elapsedtime3 = endtime3 - starttime3

"""Classifier4"""

starttime4 = time.time()

history4 = Classifier4.fit(
    train_generator, 
    steps_per_epoch = 80,
    epochs = 20, 
    validation_data = validation_generator,
    validation_steps = 20,
    verbose = 1,
    callbacks=[es1, mc4]
    )

endtime4 = time.time()
elapsedtime4 = endtime4 - starttime4

"""### Training Results

Classifier 1 Performance
"""

print('Elapsed time of Classifier 1 is {}'.format(elapsedtime1))

_, train_acc1 = Classifier1.evaluate(train_generator, verbose=0)
_, valid_acc1 = Classifier1.evaluate(validation_generator, verbose=0)
print('Train Accuracy: {}, Valid Accuracy: {}'.format(train_acc1, valid_acc1))

plt.plot(history1.history['accuracy'], label='Train')
plt.plot(history1.history['val_accuracy'], label='Val')

plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.plot(history1.history['loss'], label='Train')
plt.plot(history1.history['val_loss'], label='Val')

plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""Classifier 2 Performance"""

print('Elapsed time of Classifier 2 is {}'.format(elapsedtime2))

_, train_acc2 = Classifier2.evaluate(train_generator, verbose=0)
_, valid_acc2 = Classifier2.evaluate(validation_generator, verbose=0)
print('Train Accuracy: {}, Valid Accuracy: {}'.format(train_acc2, valid_acc2))

plt.plot(history2.history['accuracy'], label='Train')
plt.plot(history2.history['val_accuracy'], label='Val')

plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.plot(history2.history['loss'], label='Train')
plt.plot(history2.history['val_loss'], label='Val')

plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""Classifier 3 Performance"""

print('Elapsed time of Classifier 3 is {}'.format(elapsedtime3))

_, train_acc3 = Classifier3.evaluate(train_generator, verbose=0)
_, valid_acc3 = Classifier3.evaluate(validation_generator, verbose=0)
print('Train Accuracy: {}, Valid Accuracy: {}'.format(train_acc3, valid_acc3))

plt.plot(history3.history['accuracy'], label='Train')
plt.plot(history3.history['val_accuracy'], label='Val')

plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.plot(history3.history['loss'], label='Train')
plt.plot(history3.history['val_loss'], label='Val')

plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""Classifier 4 Performance"""

print('Elapsed time of Classifier 4 is {}'.format(elapsedtime4))

_, train_acc4 = Classifier4.evaluate(train_generator, verbose=0)
_, valid_acc4 = Classifier4.evaluate(validation_generator, verbose=0)
print('Train Accuracy: {}, Valid Accuracy: {}'.format(train_acc4, valid_acc4))

plt.plot(history4.history['accuracy'], label='Train')
plt.plot(history4.history['val_accuracy'], label='Val')

plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

plt.plot(history4.history['loss'], label='Train')
plt.plot(history4.history['val_loss'], label='Val')

plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""## Part 7: Test Model

### Upload Image File
"""

uploaded_file = files.upload()

uploaded_file2 = files.upload()

"""### Classifier 1"""

best_model_dir1 = '/content/best_model.h5'
model1 = load_model(best_model_dir1)

for fn in uploaded_file.keys():
  path = fn
  img = image.load_img(path, target_size=(150,150))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)
 
  images = np.vstack([x])
  classes1 = model1.predict(images, batch_size=16)
  output_class1 = np.argmax(classes1)
  
  print(fn)
  if output_class1 == 0:
    print('paper')
  elif output_class1 == 1:
    print('rock')
  else:
    print('scissors')

"""### Classifier 2"""

best_model_dir2 = '/content/best_model2.h5'
model2 = load_model(best_model_dir2)

for fn in uploaded_file.keys():
  path = fn
  img = image.load_img(path, target_size=(150,150))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)
 
  images = np.vstack([x])
  classes2 = model2.predict(images, batch_size = 32)
  output_class2 = np.argmax(classes2)
  
  print(fn)
  if output_class2 == 0:
    print('paper')
  elif output_class2 == 1:
    print('rock')
  else:
    print('scissors')

"""### Classifier 3"""

best_model_dir3 = '/content/best_model3.h5'
model3 = load_model(best_model_dir3)

for fn in uploaded_file.keys():
  path = fn
  img = image.load_img(path, target_size=(150,150))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)
 
  images = np.vstack([x])
  classes3 = model3.predict(images, batch_size = 32)
  output_class3 = np.argmax(classes3)
  
  print(fn)
  if output_class3 == 0:
    print('paper')
  elif output_class3 == 1:
    print('rock')
  else:
    print('scissors')

for fn in uploaded_file2.keys():
  path2 = fn
  img2 = image.load_img(path2, target_size=(150,150))
  imgplot2 = plt.imshow(img2)
  x2 = image.img_to_array(img2)
  x2 = np.expand_dims(x2, axis=0)
 
  images2 = np.vstack([x2])
  classes4 = model3.predict(images2, batch_size = 32)
  output_class4 = np.argmax(classes4)
  
  print(fn)
  if output_class4 == 0:
    print('paper')
  elif output_class4 == 1:
    print('rock')
  else:
    print('scissors')

"""Classifier 4"""

best_model_dir4 = '/content/best_model3.h5'
model4 = load_model(best_model_dir4)

for fn in uploaded_file.keys():
  path = fn
  img = image.load_img(path, target_size=(150,150))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)
 
  images = np.vstack([x])
  classes5 = model4.predict(images, batch_size = 32)
  output_class5 = np.argmax(classes5)
  
  print(fn)
  if output_class5 == 0:
    print('paper')
  elif output_class5 == 1:
    print('rock')
  else:
    print('scissors')

for fn in uploaded_file2.keys():
  path2 = fn
  img2 = image.load_img(path2, target_size=(150,150))
  imgplot2 = plt.imshow(img2)
  x2 = image.img_to_array(img2)
  x2 = np.expand_dims(x2, axis=0)
 
  images2 = np.vstack([x2])
  classes6 = model4.predict(images2, batch_size = 32)
  output_class6 = np.argmax(classes6)
  
  print(fn)
  if output_class6 == 0:
    print('paper')
  elif output_class6 == 1:
    print('rock')
  else:
    print('scissors')

"""## Kesimpulan

Di sini, telah dibuat 4 model image classifier dengan struktur model dan optimizer berbeda. Masing-masing model memiliki performa dan hasil pengujian yang berbeda. <br>

Dari pengujian dengan gambar background berwarna, model Classifier 1 dan Classifier 2 dapat mengklasifikasikan gambar dengan benar. Di sini, performa Classifier 2 (dengan optimizer "nadam")lebih baik dibandingkan Classifier 1 (dengan optimizer "rmsprop"). Hal ini ditandai oleh train accuracy dan validation accuracy yang lebih tinggi. Hal ini menunjukkan bahwa optimizer "nadam" lebih baik dibandingkan optimizer "rmsprop". <br>

Sayangnya, Classifer 3 dan Classifier 4 tidak mampu memprediksi gambar dengan background selain hijau. Di sini, optimizer yang digunakan adalah "adamax", tetapi dengan struktur model yang dibuat berbeda. Kedua model ini memiliki akurasi yang cukup tinggi, dengan best_model memiliki val_accuracy = 1. Meskipun begitu, hal ini tidak menjamin model bisa memprediksi gambar backrgound non-hijau. Dari grafik akurasi dan loss, kemungkinan kedua model masih terhitung underfit, karena trainingnya terhenti oleh Early Stopping demi menghemat waktu komputasi. <br>

Dengan demikian, dapat disimpulkan bahwa model terbaik adalah Classifier 2 dengan optimizer nadam. Dengan nilai <br> Train Accuracy = 0.9832572340965271 
<br> Valid Accuracy: 0.991990864276886 
<br> Elapsed Time =  1298.0829107761383 sekon atau 21.63 menit
"""