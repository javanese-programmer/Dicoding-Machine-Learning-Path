# -*- coding: utf-8 -*-
"""Proyek_Pertama.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xZ1dIVvY9IWexXok7Xubzpfm_X-x8Wm7

# NLP Model for Hate Speech Classification

## Dev's Profile

Nama : Tariq Fitria Aziz <br>
Bergabung: 02 September 2020 <br>
Asal: Kabupaten Wonogiri, Jawa Tengah

## Problem Desc

Di sini, akan dibuat sebuah model Natural Language Processing (NLP) yang dapat digunakan untuk mengklasifikasikan tweet tentang Coronavirus, berdasarkan sentimennya. <br>

Dataset yang digunakan dapat dilihat di sumber berikut: https://www.kaggle.com/datatattle/covid-19-nlp-text-classification

## Part I: Preparation

### Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
import chardet
import string

import nltk
import spacy
from nltk.corpus import stopwords

import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ModelCheckpoint

from sklearn.model_selection import train_test_split

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
# %matplotlib inline

"""### Set Drive Folder"""

drive.mount('/content/drive/')

"""## Part II: Data Preparation

### Data Loading
"""

data_train = pd.read_csv('/content/drive/MyDrive/Datasets/Corona_NLP_train.csv', encoding='latin-1')
data_train.head(5)

data_test = pd.read_csv('/content/drive/MyDrive/Datasets/Corona_NLP_test.csv', encoding='latin-1')
data_test.head(5)

"""### Data Information"""

data_train.info()

data_test.info()

"""### Data Preprocessing

Drop Unneeded Labels
"""

columns = ['UserName', 'ScreenName', 'Location', 'TweetAt']
data_train2 = data_train.drop(columns = columns)

data_test2 = data_test.drop(columns = columns)

"""Change Labels Value"""

data_train2['Sentiment'].unique()

data_train2['Sentiment'] = data_train2['Sentiment'].replace('Extremely Positive', 'Positive')
data_train2['Sentiment'] = data_train2['Sentiment'].replace('Extremely Negative', 'Negative')

data_train2['Sentiment'].unique()

data_test2['Sentiment'] = data_test2['Sentiment'].replace('Extremely Positive', 'Positive')
data_test2['Sentiment'] = data_test2['Sentiment'].replace('Extremely Negative', 'Negative')

data_test2['Sentiment'].unique()

"""One-Hot-Encoding"""

cat = pd.get_dummies(data_train2['Sentiment'])
data_train3 = pd.concat([data_train2, cat], axis=1)
data_train3 = data_train3.drop(columns='Sentiment')
data_train3.head(10)

cat2 = pd.get_dummies(data_test2['Sentiment'])
data_test3 = pd.concat([data_test2, cat2], axis=1)
data_test3 = data_test3.drop(columns='Sentiment')
data_test3.head(10)

"""### Data Split"""

text0 = data_train3['OriginalTweet'].values
labels0 = data_train3.iloc[:, 1:].values

text_latih, text_valid, labels_latih, labels_valid = train_test_split(text0, labels0, test_size = 0.13)

text_valid2 = data_test3['OriginalTweet'].values
labels_valid2 = data_test3.iloc[:, 1:].values

text_valid = np.concatenate((text_valid, text_valid2), axis = 0)

labels_valid = np.concatenate((labels_valid, labels_valid2), axis = 0)

text_valid.shape

text_latih.shape

"""## Part III: Text Preprocessing

### Text Cleaning
"""

nltk.download('stopwords')

def clean_text(teks):
  new_teks = []
  teks = teks.tolist()
  for i in teks:
    cleaned = i.lower()

    unknown = ['â\x97', 'â\x96', 'â\x92', 'USER', '\r', '\n', '\\n', '\\xf0', 
               '\\x9f', '\\x98', '\\xaa', 'https://t.co/', '&gt;', '.g']
    for j in unknown:
      cleaned = cleaned.replace(j, '')
    
    for k in range(0,10):
      cleaned = cleaned.replace('{}'.format(k), '')
    
    tanda_baca = ['"','%','&','(',')','*','+',',','-','.',';','<','=','>','[',']',
                  '^','_',',','{','|','}','~','`', '//', '/', ':', '!', '?', '#',
                  '$', '@']
    for l in tanda_baca:
      cleaned = cleaned.replace(l, '')
    
    cleaned = cleaned.strip()
    
    #stopword_list = stopwords.words('english')
    words_in = cleaned.split()
    #for m in stopword_list:
    # words_in = ['' if x == m else x for x in words_in]
    
    alphabet = list(string.ascii_lowercase)
    for n in alphabet:
      words_in = ['' if x == n else x for x in words_in]

    separator = ' '
    cleaned = separator.join(words_in)

    new_teks.append(cleaned)
  
  new_teks = np.array(new_teks)
  return new_teks

text_latih = clean_text(text_latih)
text_valid = clean_text(text_valid)

"""### Tokenization"""

tokenizer = Tokenizer(num_words = 5000, oov_token='-', filters='\'')

tokenizer.fit_on_texts(text_latih)
tokenizer.fit_on_texts(text_valid)

tokens = tokenizer.word_index
tokens

sekuens_latih = tokenizer.texts_to_sequences(text_latih)
sekuens_valid = tokenizer.texts_to_sequences(text_valid)

"""### Sequences Padding"""

padded_sekuens_latih = pad_sequences(sekuens_latih)
padded_sekuens_valid = pad_sequences(sekuens_valid)

"""## Part IV: Model Creation

### Function Definition
"""

def initialize_model(name, optimizer):
  model = Sequential(name = name)

  model.add(Embedding(
      input_dim=5000, 
      output_dim=32
      )
  )
  model.add(LSTM(
      units = 256,
      dropout = 0.5,
      )
  )
  model.add(Dense(
      units = 512,
      activation = 'relu',
      kernel_initializer = 'he_uniform'
      )
  )
  model.add(Dropout(
      rate = 0.5
      )
  )
  model.add(Dense(
      units = 256,
      activation = 'relu',
      kernel_initializer = 'he_uniform'
      )
  )
  model.add(Dropout(
      rate = 0.5
      )
  )
  model.add(Dense(
      units = 128,
      activation = 'relu',
      kernel_initializer = 'he_uniform'
      )
  )
  model.add(Dropout(
      rate = 0.5
      )
  )
  model.add(Dense(
      units = 3,
      activation = 'softmax',
      )
  )

  model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])

  return model

"""### Model Initialization"""

Processor1 = initialize_model('Processor1', 'nadam')

"""### Model Summary"""

Processor1.summary()

"""## Part V: Train Model

### Define Callbacks
"""

es1 = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc1 = ModelCheckpoint('best_model1.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)

"""### Train Process"""

history1 = Processor1.fit(padded_sekuens_latih, labels_latih, epochs = 25, 
                          validation_data=(padded_sekuens_valid, labels_valid), 
                          batch_size = 32, callbacks = [es1, mc1], verbose=1
                          )

"""### Training Results"""

plt.plot(history1.history['loss'], label='loss')
plt.plot(history1.history['val_loss'], label='val_loss')

plt.title("Plot Traing vs Validation Loss")
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.show()

plt.plot(history1.history['accuracy'], label='accuracy')
plt.plot(history1.history['val_accuracy'], label='val_accuracy')

plt.title("Plot Traing vs Validation Accuracy")
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.show()